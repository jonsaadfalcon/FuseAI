nohup: ignoring input
05/08/2024 05:33:59 - INFO - __main__ - Data processing args: Namespace(model_name_or_path='openai-community/gpt2-xl', dataset='datasets/minipile_processed', dataset_save_dir='datasets/minipile_processed_lm_representations/gpt2-xl/1_4/', dataset_sample_prop=None, dataset_split_num=4, dataset_index=1, cache_dir='datasets/minipile_processed', model_max_length=2048, training_mode='full', load_in_half='bf16', batch_size=16, preprocessing_num_workers=128, top_k_logits=10, save_per_token_metric=True, no_assert=False)
05/08/2024 05:33:59 - INFO - __main__ - Split num: 4; Split index: 1.
05/08/2024 05:33:59 - INFO - __main__ - train: start_index: 381941, end_index: 763882, select_size: 381941
05/08/2024 05:33:59 - INFO - __main__ - validation: start_index: 184, end_index: 368, select_size: 184
05/08/2024 05:33:59 - INFO - __main__ - test: start_index: 3772, end_index: 7544, select_size: 3772
05/08/2024 05:33:59 - INFO - __main__ - Loading tokenizer.
05/08/2024 05:34:00 - INFO - __main__ - bos_token: <|endoftext|>, 50256 eos_token: <|endoftext|>, 50256 unk_token: <|endoftext|>, 50256 pad_token: <|endoftext|>, 50256 
Tokenize the dataset. (num_proc=128):   0%|          | 0/381941 [00:00<?, ? examples/s]Tokenize the dataset. (num_proc=128):   0%|          | 1000/381941 [00:04<28:52, 219.93 examples/s]Tokenize the dataset. (num_proc=128):   1%|▏         | 5000/381941 [00:04<04:26, 1414.80 examples/s]Tokenize the dataset. (num_proc=128):   2%|▏         | 9000/381941 [00:04<02:04, 3004.35 examples/s]Tokenize the dataset. (num_proc=128):   3%|▎         | 13000/381941 [00:04<01:15, 4875.24 examples/s]Tokenize the dataset. (num_proc=128):   4%|▍         | 17000/381941 [00:05<00:51, 7136.21 examples/s]Tokenize the dataset. (num_proc=128):   6%|▌         | 22000/381941 [00:05<00:34, 10514.25 examples/s]Tokenize the dataset. (num_proc=128):   7%|▋         | 25000/381941 [00:05<00:28, 12331.88 examples/s]Tokenize the dataset. (num_proc=128):   7%|▋         | 28000/381941 [00:05<00:25, 14067.76 examples/s]Tokenize the dataset. (num_proc=128):   9%|▊         | 33000/381941 [00:05<00:18, 19195.05 examples/s]Tokenize the dataset. (num_proc=128):  10%|▉         | 37000/381941 [00:05<00:21, 16224.48 examples/s]Tokenize the dataset. (num_proc=128):  10%|█         | 40000/381941 [00:06<00:19, 17612.82 examples/s]Tokenize the dataset. (num_proc=128):  12%|█▏        | 45000/381941 [00:06<00:15, 21361.85 examples/s]Tokenize the dataset. (num_proc=128):  13%|█▎        | 49000/381941 [00:06<00:14, 23740.38 examples/s]Tokenize the dataset. (num_proc=128):  15%|█▍        | 57000/381941 [00:06<00:09, 34087.94 examples/s]Tokenize the dataset. (num_proc=128):  16%|█▌        | 62000/381941 [00:06<00:09, 32417.86 examples/s]Tokenize the dataset. (num_proc=128):  18%|█▊        | 67000/381941 [00:06<00:08, 35511.80 examples/s]Tokenize the dataset. (num_proc=128):  19%|█▉        | 72000/381941 [00:07<00:11, 27286.19 examples/s]Tokenize the dataset. (num_proc=128):  20%|██        | 77000/381941 [00:07<00:09, 31119.54 examples/s]Tokenize the dataset. (num_proc=128):  21%|██        | 81000/381941 [00:07<00:11, 25705.66 examples/s]Tokenize the dataset. (num_proc=128):  23%|██▎       | 87000/381941 [00:07<00:09, 32117.97 examples/s]Tokenize the dataset. (num_proc=128):  24%|██▍       | 91000/381941 [00:07<00:10, 26597.77 examples/s]Tokenize the dataset. (num_proc=128):  25%|██▌       | 96000/381941 [00:07<00:09, 30888.80 examples/s]Tokenize the dataset. (num_proc=128):  26%|██▌       | 100000/381941 [00:07<00:10, 27806.65 examples/s]Tokenize the dataset. (num_proc=128):  28%|██▊       | 106000/381941 [00:08<00:08, 31859.77 examples/s]Tokenize the dataset. (num_proc=128):  29%|██▉       | 110000/381941 [00:08<00:08, 33028.48 examples/s]Tokenize the dataset. (num_proc=128):  30%|██▉       | 114000/381941 [00:08<00:07, 34298.75 examples/s]Tokenize the dataset. (num_proc=128):  31%|███       | 118000/381941 [00:08<00:09, 2735